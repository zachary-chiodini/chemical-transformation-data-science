{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, re\n",
    "import numpy as np\n",
    "from math import e\n",
    "from typing import Dict, List, Set\n",
    "from nptyping import NDArray, Float64\n",
    "from Bio import Entrez, Medline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open( 'positive_examples.txt' ) as file :\n",
    "    posids = set( file.read().split( ',' ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open( 'negative_examples.txt' ) as file :\n",
    "    negids = set( file.read().split( ',' ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Count = int\n",
    "Class, Document, Word = str, str, str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data : Dict[ Class, Set[ Document ] ] = { 'positive' : set(), 'negative' : set() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting data\n",
    "for pmid in posids :\n",
    "    data[ 'positive' ].add(\n",
    "        Medline.read(\n",
    "            Entrez.efetch(\n",
    "                db = 'pubmed',\n",
    "                id = pmid,\n",
    "                email = 'chiodini.zachary@epa.gov',\n",
    "                retmode = 'text',\n",
    "                rettype = 'medline'\n",
    "                )\n",
    "            ).get( 'AB' )\n",
    "        )\n",
    "    time.sleep( 1/3 ) # avoid PubMed ban\n",
    "for pmid in negids :\n",
    "    data[ 'negative' ].add(\n",
    "        Medline.read(\n",
    "            Entrez.efetch(\n",
    "                db = 'pubmed',\n",
    "                id = pmid,\n",
    "                email = 'chiodini.zachary@epa.gov',\n",
    "                retmode = 'text',\n",
    "                rettype = 'medline'\n",
    "                )\n",
    "            ).get( 'AB' )\n",
    "        )\n",
    "    time.sleep( 1/3 ) # avoid PubMed ban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression :\n",
    "    '''\n",
    "    Binomial Logistic Regression for Text Classification\n",
    "    '''\n",
    "    def __init__( self ) :\n",
    "        self.network = np.array([])    # weights and bias\n",
    "        self.vocabulary = np.array([]) # features\n",
    "        self.__vecfindall = np.vectorize( \n",
    "            lambda voc, doc : int( voc in doc ) \n",
    "            )\n",
    "        self.stop_words = {}\n",
    "        with open( 'stopset.txt' ) as file :\n",
    "            self.stop_words = set( file.read().split( ',' ) )\n",
    "            \n",
    "    def train( self, \n",
    "        data : Dict[ Class, Set[ Document ] ],\n",
    "        rate : float = 1.0,\n",
    "        batches : int = 10,\n",
    "        convergence : float = 0.01,\n",
    "        ) -> None :\n",
    "        # get features\n",
    "        self.vocabulary = np.array( list( \n",
    "            self.__removeFirstQ( self.__extract( data ) )\n",
    "            ) )\n",
    "        # append bias\n",
    "        self.vocabulary = np.append( self.vocabulary, '' )\n",
    "        # generate random weights\n",
    "        self.network = np.random.uniform( \n",
    "            -0.5, 0.5, size = len( self.vocabulary )\n",
    "            )\n",
    "        # generate input vectors\n",
    "        X = np.empty( shape = ( 0, len( self.vocabulary ) ) )\n",
    "        for label in data :\n",
    "            for document in data[ label ] :\n",
    "                X = np.vstack( ( X, self.getInputFrom( document ) ) )\n",
    "        # get target values\n",
    "        target = list( data )[ 0 ]\n",
    "        Y = np.array( [ int( label == target ) for label in data \n",
    "                        for document in data[ label ] ] )\n",
    "        totgrad = np.inf\n",
    "        while totgrad > convergence :\n",
    "            totgrad = 0\n",
    "            for xbatch, ybatch in zip( \n",
    "                np.array_split( X, batches ), \n",
    "                np.array_split( Y, batches ) \n",
    "                ) :\n",
    "                grad = np.multiply( \n",
    "                    np.reshape( \n",
    "                        self.output( xbatch ) - ybatch, \n",
    "                        newshape = ( len( xbatch ), 1 )\n",
    "                        ),\n",
    "                    xbatch\n",
    "                    ).sum( axis = 0 ) / len( xbatch )\n",
    "                self.network = self.network - rate*grad\n",
    "                totgrad += grad.sum() / len( grad )\n",
    "        return\n",
    "    \n",
    "    def test( self, \n",
    "        data : Dict[ Class, Set[ Document ] ], \n",
    "        boundary : float = 0.5 \n",
    "    ) -> None :\n",
    "        pass\n",
    "            \n",
    "    def getInputFrom( self, document : str ) -> NDArray[ int ] :\n",
    "        ''' Generate Input Vector '''\n",
    "        return self.__findall( self.vocabulary, document )\n",
    "            \n",
    "    def output( self, X : NDArray[ int ] ) :\n",
    "        ''' Logistic Model Output '''\n",
    "        return self.sigmoid( np.dot( X, self.network ) )\n",
    "            \n",
    "    def sigmoid( self, x : float ) -> float :\n",
    "        return 1 / ( 1 + e**(-x) )\n",
    "    \n",
    "    def __findall( self, \n",
    "        vocabulary : NDArray[ Word ], \n",
    "        document   : str\n",
    "        ) -> NDArray[ int ] :\n",
    "        return self.__vecfindall( vocabulary, document )\n",
    "\n",
    "    def __extract( self,\n",
    "        data : Dict[ Class, Set[ Document ] ],\n",
    "        pattern : str = '\\\\b[a-z]{2,}\\\\b'\n",
    "        ) -> Dict[ Word, Count ] :\n",
    "        '''\n",
    "        Extract Vocabulary from Dataset\n",
    "        '''\n",
    "        vocabulary : Dict[ Word, Count ] = {}\n",
    "        for label in data :\n",
    "            for document in data[ label ] :\n",
    "                for word in re.findall(\n",
    "                    pattern = pattern,\n",
    "                    string  = document\n",
    "                    ) :\n",
    "                    if word not in self.stop_words :\n",
    "                        if word in vocabulary :\n",
    "                            vocabulary[ word ] += 1\n",
    "                        else :\n",
    "                            vocabulary[ word ] = 1\n",
    "        return vocabulary\n",
    "    \n",
    "    def __removeFirstQ( self,\n",
    "        vocabulary : Dict[ Word, Count ]\n",
    "        ) -> Dict[ Word, Count ] :\n",
    "        '''\n",
    "        Remove First Quartile in Vocabulary\n",
    "        '''\n",
    "        count = sorted( set( vocabulary.values() ) )\n",
    "        index = len( count ) / 4\n",
    "        if index % 1 == 0 :\n",
    "            limit = count[ int( index ) ]\n",
    "        else :\n",
    "            index = int( index ) # truncate\n",
    "            limit = ( count[ index ] + count[ index + 1 ] ) / 2\n",
    "        for word, count in vocabulary.copy().items() :\n",
    "            if count < limit :\n",
    "                del vocabulary[ word ]\n",
    "        return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train( data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05063314, -0.7412253 ,  0.77752734, -0.0122118 , -0.40372418,\n",
       "       -0.00409059,  0.81301947,  0.18197832,  0.79984464,  0.43732911,\n",
       "       -0.41077634,  0.94513105,  0.12216768,  0.05564902,  0.68189066,\n",
       "        0.39209664, -0.31632403, -0.46583514,  0.27344065, -0.19113349,\n",
       "        0.06174579,  0.98470539, -0.25166816,  0.87969516,  0.24531371,\n",
       "        0.81272763, -0.01590201,  0.10830178, -0.52954956, -0.3696967 ,\n",
       "        0.5619696 ,  0.10280133, -0.11656916, -0.06118743,  0.49972869,\n",
       "        1.11599386,  0.20016204, -0.04948729, -0.40842085, -0.46140762,\n",
       "       -0.42006598, -0.02340598,  0.07638705, -0.29508413, -0.03422346,\n",
       "       -0.04968193,  0.47553087,  0.60268718,  0.37937401, -0.89076507,\n",
       "       -0.00307398,  0.13066231,  0.42321262,  0.03048048,  0.24700911,\n",
       "       -0.12092759,  0.06873496, -0.28533924, -0.47837386, -0.09516471,\n",
       "       -0.15620561,  0.39970991,  0.05045602, -0.20935676,  0.14729556,\n",
       "        0.30204452, -0.41596891,  0.21015156,  0.74266737,  0.10684956,\n",
       "       -0.49456157, -0.0171695 , -0.32480458, -0.07795462,  0.49290867,\n",
       "       -0.24513669,  0.58856285,  0.26349386,  0.01158271,  0.16260601,\n",
       "       -0.04234307,  0.54625107, -0.11159447,  0.00439633, -0.02825741,\n",
       "       -0.08695293, -0.09413721, -0.11156683, -0.66775422,  0.22104932,\n",
       "        0.32218756,  0.15111929, -0.01433147, -0.30463362, -0.12727253,\n",
       "        0.03180328, -0.32836791,  0.08903934, -0.08763392, -0.43322951,\n",
       "       -0.08739343,  0.23209928, -0.46936403, -0.05703638,  0.15336828,\n",
       "        0.05875269, -0.52685284,  0.09143964, -0.10861235, -0.46241866,\n",
       "       -0.45284515,  0.17151503, -0.4840965 , -0.51855732, -0.76785937,\n",
       "       -0.23368306,  0.3694324 , -0.6851123 ,  0.15272541, -0.08280968,\n",
       "       -0.18960632, -0.44054296,  0.06283878, -0.4433326 , -0.1217608 ,\n",
       "       -0.39187043,  0.15191907,  0.15405202, -0.00884099,  0.30092447,\n",
       "        0.11668026, -0.14606394, -0.3383527 , -0.85088658, -0.30790397,\n",
       "       -0.62971117, -0.07040026, -0.63184448,  0.02017454, -0.76778827,\n",
       "       -0.7425986 , -0.14475149, -0.00523308, -0.63634495, -0.41172773,\n",
       "       -0.42848103, -0.08014356, -0.16998247, -0.2810042 , -0.6488635 ,\n",
       "        0.2493464 , -0.08126589, -0.166449  , -0.01968049, -0.00157241,\n",
       "       -0.38075051, -0.2597568 ,  0.24533757, -0.27715656, -0.24171479,\n",
       "       -0.0291074 ,  0.39594879, -0.30157431, -0.18588499,  0.39731019,\n",
       "       -0.5985521 ])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
