{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, re\n",
    "import numpy as np\n",
    "from math import e\n",
    "from typing import Dict, List, Set\n",
    "from nptyping import NDArray, Float64\n",
    "from Bio import Entrez, Medline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open( 'positive_examples.txt' ) as file :\n",
    "    posids = set( file.read().split( ',' ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open( 'negative_examples.txt' ) as file :\n",
    "    negids = set( file.read().split( ',' ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Count = int\n",
    "Class, Document, Word = str, str, str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data : Dict[ Class, Set[ Document ] ] = { 'positive' : set(), 'negative' : set() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting data\n",
    "for pmid in posids :\n",
    "    data[ 'positive' ].add(\n",
    "        Medline.read(\n",
    "            Entrez.efetch(\n",
    "                db = 'pubmed',\n",
    "                id = pmid,\n",
    "                email = 'chiodini.zachary@epa.gov',\n",
    "                retmode = 'text',\n",
    "                rettype = 'medline'\n",
    "                )\n",
    "            ).get( 'AB' )\n",
    "        )\n",
    "    time.sleep( 1/3 ) # avoid PubMed ban\n",
    "for pmid in negids :\n",
    "    data[ 'negative' ].add(\n",
    "        Medline.read(\n",
    "            Entrez.efetch(\n",
    "                db = 'pubmed',\n",
    "                id = pmid,\n",
    "                email = 'chiodini.zachary@epa.gov',\n",
    "                retmode = 'text',\n",
    "                rettype = 'medline'\n",
    "                )\n",
    "            ).get( 'AB' )\n",
    "        )\n",
    "    time.sleep( 1/3 ) # avoid PubMed ban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression :\n",
    "    '''\n",
    "    Binomial Logistic Regression for Text Classification\n",
    "    '''\n",
    "    def __init__( self ) :\n",
    "        self.network = np.array([])    # weights and bias\n",
    "        self.vocabulary = np.array([]) # features\n",
    "        self.__vecfindall = np.vectorize( \n",
    "            lambda voc, doc : int( voc in doc ) \n",
    "            )\n",
    "        self.stop_words = {}\n",
    "        with open( 'stopset.txt' ) as file :\n",
    "            self.stop_words = set( file.read().split( ',' ) )\n",
    "            \n",
    "    def train( self, \n",
    "        data : Dict[ Class, Set[ Document ] ],\n",
    "        rate : float = 1.0,\n",
    "        batches : int = 10,\n",
    "        convergence : float = 0.01,\n",
    "        ) -> None :\n",
    "        # get features\n",
    "        self.vocabulary = np.array( list( \n",
    "            self.__removeFirstQ( self.__extract( data ) )\n",
    "            ) )\n",
    "        # append bias\n",
    "        self.vocabulary = np.append( self.vocabulary, '' )\n",
    "        # generate random weights\n",
    "        self.network = np.random.uniform( \n",
    "            -0.5, 0.5, size = len( self.vocabulary )\n",
    "            )\n",
    "        # generate input vectors\n",
    "        X = np.empty( shape = ( 0, len( self.vocabulary ) ) )\n",
    "        for label in data :\n",
    "            for document in data[ label ] :\n",
    "                X = np.vstack( ( X, self.getInputFrom( document ) ) )\n",
    "        # get target values\n",
    "        target = list( data )[ 0 ]\n",
    "        Y = np.array( [ int( label == target ) for label in data \n",
    "                        for document in data[ label ] ] )\n",
    "        totgrad = np.inf\n",
    "        while totgrad > convergence :\n",
    "            totgrad = 0\n",
    "            for xbatch, ybatch in zip( \n",
    "                np.array_split( X, batches ), \n",
    "                np.array_split( Y, batches ) \n",
    "                ) :\n",
    "                grad = np.multiply( \n",
    "                    np.reshape( \n",
    "                        self.output( xbatch ) - ybatch, \n",
    "                        newshape = ( len( batch ), 1 )\n",
    "                        ),\n",
    "                    xbatch\n",
    "                    ).sum( axis = 0 ) / len( xbatch )\n",
    "                self.network = self.network - rate*grad\n",
    "                totgrad += grad.sum() / len( grad )\n",
    "        return\n",
    "    \n",
    "    def test( self, \n",
    "        data : Dict[ Class, Set[ Document ] ], \n",
    "        boundary : float = 0.5 \n",
    "    ) -> None :\n",
    "        pass\n",
    "            \n",
    "    def getInputFrom( self, document : str ) -> NDArray[ int ] :\n",
    "        ''' Generate Input Vector '''\n",
    "        return self.__findall( self.vocabulary, document )\n",
    "            \n",
    "    def output( self, X : NDArray[ int ] ) :\n",
    "        ''' Logistic Model Output '''\n",
    "        return self.sigmoid( np.dot( X, self.network ) )\n",
    "            \n",
    "    def sigmoid( self, x : float ) -> float :\n",
    "        return 1 / ( 1 + e**(-x) )\n",
    "    \n",
    "    def __findall( self, \n",
    "        vocabulary : NDArray[ Word ], \n",
    "        document   : str\n",
    "        ) -> NDArray[ int ] :\n",
    "        return self.__vecfindall( vocabulary, document )\n",
    "\n",
    "    def __extract( self,\n",
    "        data : Dict[ Class, Set[ Document ] ],\n",
    "        pattern : str = '\\\\b[a-z]{2,}\\\\b'\n",
    "        ) -> Dict[ Word, Count ] :\n",
    "        '''\n",
    "        Extract Vocabulary from Dataset\n",
    "        '''\n",
    "        vocabulary : Dict[ Word, Count ] = {}\n",
    "        for label in data :\n",
    "            for document in data[ label ] :\n",
    "                for word in re.findall(\n",
    "                    pattern = pattern,\n",
    "                    string  = document\n",
    "                    ) :\n",
    "                    if word not in self.stop_words :\n",
    "                        if word in vocabulary :\n",
    "                            vocabulary[ word ] += 1\n",
    "                        else :\n",
    "                            vocabulary[ word ] = 1\n",
    "        return vocabulary\n",
    "    \n",
    "    def __removeFirstQ( self,\n",
    "        vocabulary : Dict[ Word, Count ]\n",
    "        ) -> Dict[ Word, Count ] :\n",
    "        '''\n",
    "        Remove First Quartile in Vocabulary\n",
    "        '''\n",
    "        count = sorted( set( vocabulary.values() ) )\n",
    "        index = len( count ) / 4\n",
    "        if index % 1 == 0 :\n",
    "            limit = count[ int( index ) ]\n",
    "        else :\n",
    "            index = int( index ) # truncate\n",
    "            limit = ( count[ index ] + count[ index + 1 ] ) / 2\n",
    "        for word, count in vocabulary.copy().items() :\n",
    "            if count < limit :\n",
    "                del vocabulary[ word ]\n",
    "        return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
